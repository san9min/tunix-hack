{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üëã Introduction\n",
    "\n",
    "---\n",
    "\n",
    "### Google Tunix Hack\n",
    "*Train a model to show its work*\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/san9min/tunix-hack/blob/main/assets/profile.png?raw=true\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "This project targets the Google Tunix Hack ‚Äì Train a model to show its work by proposing a post-training pipeline that enables small language models (SLMs) to produce explicit and consistent reasoning traces. \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/san9min/tunix-hack/main/assets/post-training-pipeline.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Our approach first applies LoRA-based supervised fine-tuning (SFT) on a teacher-distilled, high-quality general-reasoning dataset to stabilize output format and reasoning behavior. We then perform LoRA-based GRPO reinforcement learning with rule- and probability-based rewards on a general reasoning dataset to improve reasoning ability.\n",
    "\n",
    "This work demonstrates a practical recipe for training small reasoning models under limited compute constraints, aligned with Tunix‚Äôs goal of making step-by-step reasoning accessible to the open-source community.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Training Strategy\n",
    "---\n",
    "We select Gemma-2B-IT from the small Gemma family due to its instruction-tuned initialization.\n",
    "\n",
    "## üë®‚Äçüè´ Synthetic Data SFT Distillation\n",
    "\n",
    "We adopt LoRA-based SFT as an initial stage to stabilize the model‚Äôs reasoning structure. The SFT process is conducted in two stages. In the first stage, the model is trained on general-domain data containing relatively long chains of thought (CoT) to learn reasoning processes. In the second stage, we further fine-tune the model on samples with reasoning lengths of up to 1024 tokens, which is our target length, while slightly increasing the proportion of mathematical reasoning data.\n",
    "\n",
    "\n",
    "## ‚öñÔ∏è RL with GRPO using Rule- & Probability-Based Rewards\n",
    "\n",
    "We apply Group Relative Policy Optimization (GRPO) to adaptively improve reasoning quality using relative comparisons among sampled responses.\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "<div align=\"center\">\n",
    "      <img src=\"https://github.com/san9min/tunix-hack/blob/main/assets/reward_function_pipeline.png?raw=true\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "We use a domain-routed hybrid reward with a shared format constraint across all domains.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* Math samples receive format + answer correctness rewards.\n",
    "\n",
    "* Non-math samples receive format + probability-based (RLPR-style) rewards.\n",
    "\n",
    "\n",
    "### Rule-based Reward $R_{\\text{rule}}$\n",
    "\n",
    "We define a rule-based reward composed of two components:\n",
    "a format reward, which is applied to all domains, and an answer reward, which is only applied to math samples.\n",
    "\n",
    "The rule-based reward for math domain is defined as:\n",
    "$$ R_{rule} = R_{format} + R_{answer} $$\n",
    "  \n",
    "  - **Format Reward $R_{\\text{fmt}}$** : This reward is applied to both math and non-math samples, enforcing a strict output contract: `<reasoning> ... </reasoning> <answer> ... </answer>`.A response receives a positive reward only if it strictly matches the required tag template, with no extra text outside the tags, correct tag order, non-empty reasoning and answer spans. Missing tags, malformed structure, strict-match failures, or empty spans receive negative penalties.\n",
    "\n",
    "  - **Answer Reward $R_{\\text{ans}}$** : ONLY for Math domain. Answer correctness is evaluated using a cascade of increasingly permissive checks:\n",
    "      - Exact string match after normalization (whitespace, currency symbols, commas)\n",
    "      - Numeric parsing for valid numeric forms including integers,floating-point / scientific notation, fractions,percentages\n",
    "      - Ratio-band partial credit when numeric parsing succeeds but values differ, using: $[0.9, 1.1]$, then $[0.8, 1.2]$, with decreasing reward.If direct parsing fails but the ground-truth answer is numeric, we extract the last numeric token from the model output and provide limited partial credit.\n",
    "\n",
    "\n",
    "### Probability-based Reward $R_{\\text{prob}}$ (Non-Math)\n",
    "\n",
    "For non-math samples, we compute a policy-confidence signal by measuring the rollout policy‚Äôs average probability of the ground-truth answer tokens under teacher forcing.\n",
    "\n",
    "Let $y*$ denote the ground-truth answer (optionally wrapped as `<answer>`{y*}`</answer>` and appended with EOS), and let $\\mathcal{Y}$ be the token positions of $y*$.\n",
    "\n",
    "We compute per-token probabilities under the rollout policy $\\pi$ as:\n",
    "\n",
    "$$ R_{\\text{full}}= \\frac{1}{|\\mathcal{Y}|}\\sum_{t \\in \\mathcal{Y}} \\exp\\!\\left(\\log p_{\\pi}\\left(y_t^{*}\\mid\\text{prompt} + \\text{reasoning},y_{<t}^{*}\\right)\\right).$$\n",
    "\n",
    "\n",
    "**Debiasing**\n",
    "\n",
    "We also compute a prompt-only baseline to define the final prbability reward as $$R_{prob}=Clip(R_{full}‚àíR_{base},[0,1])$$ \n",
    "\n",
    "**Std-based Prompt Filtering**\n",
    "\n",
    "When sampling $G$ rollouts per prompt, we compute the per-prompt standard deviation of $R_{\\text{prob}}$.\n",
    "Prompts with standard deviation below a threshold $\\beta$ (fixed or EMA-updated) are filtered by zeroing out their $R_{\\text{prob}}$.\n",
    "\n",
    "**Objective**\n",
    "\n",
    "The final reward combines a **shared format reward** with a\n",
    "**domain-specific learning signal** (answer correctness for math, probability for non-math):\n",
    "\n",
    "For each sample $i$, the final reward is:\n",
    "\n",
    "$$R_i=\\begin{cases}\\lambda_{\\text{rule}} \\, R_{\\text{rule}, i},& \\text{if } d_i = \\text{math}, \\\\[6pt]\\lambda_{\\text{prob}} \\, R_{\\text{prob}, i}+\\lambda_{\\text{fmt}} \\, R_{\\text{fmt}, i},& \\text{otherwise}.\\end{cases}$$\n",
    "\n",
    "where $d_i \\in ( \\text{math}, \\text{non-math})$ denote the per-sample domain label.\n",
    "\n",
    "*(Optionally, the probability term can be **gated on strict format success** for non-math samples;  \n",
    "if the strict tag format fails, $R_{\\text{prob}}$ is dropped and only the format reward is applied.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Finetuning Dataset\n",
    "---\n",
    "## üìù SFT Dataset\n",
    "\n",
    "We construct our SFT dataset by curating and filtering subsets from publicly available reasoning datasets, including `glaiveai/reasoning-v1-20m` and `bespokelabs/Bespoke-Stratos-17k`. Glaiveai supplies large-scale general-domain reasoning traces beyond math and code, while Bespoke complements it with high-quality, correctness-filtered reasoning data across math, code, science, and logic tasks.\n",
    "\n",
    "We strictly preprocess the dataset by enforcing standardized reasoning/answer tag formats, correct structure and uniqueness, token budget limits, removal of malformed samples, and English-only content to ensure training stability and format reliability.\n",
    "\n",
    "Under constrained computational resources, we further reduce the dataset size through domain- and CoT length-aware sampling. Specifically, we sample reasoning traces using a 1:4 ratio between long and short CoT examples, aiming to balance exposure to rich reasoning processes‚Äîsuch as self-correction and verification patterns typically found in long CoTs‚Äîwith shorter reasoning traces that are more learnable for small models.\n",
    "\n",
    "For SFT Stage 2, we refine this dataset by retaining only samples with reasoning lengths below 1024 tokens and slightly increasing the proportion of mathematical reasoning tasks.\n",
    "\n",
    "<div align=\"center\">\n",
    "      <img src=\"https://github.com/san9min/tunix-hack/blob/main/assets/sft_dataset_response_distribution.webp?raw=true\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "You can use our this at here:\n",
    "\n",
    "* SFT Stage1 Dataset : [DOD Gemma2 SFT Dataset](https://www.kaggle.com/datasets/sangminlee09/small-reasoning-model-sft-dataset)\n",
    "* SFT Stage2 Dataset : [DOD Gemma2 SFT subset](https://www.kaggle.com/datasets/sangminlee09/small-reasoning-model-sft-subset)\n",
    "\n",
    "\n",
    "## üìö RL Dataset\n",
    "\n",
    "The RL training dataset is constructed by integrating multiple reasoning-oriented sources to ensure broad general-domain coverage.\n",
    "\n",
    "Tasks with strictly rule-verifiable answers are collected from the `knoveleng/open-rs` dataset. We retain only samples with unambiguous numeric ground-truth answers, restricting valid formats to integers, floating-point numbers, and rational fractions, including both plain-text (e.g., 7/24) and LaTeX-style representations (e.g., \\frac{7}{24}).\n",
    "\n",
    "Additional general-domain reasoning tasks are drawn from two sources. Creative, preference-driven tasks are sourced from `SAA-Lab/LitBench-Train`, where samples are ranked by user feedback metadata. Non-creative reasoning tasks spanning domains such as science and business are collected from the `openbmb/RLPR-Train-Dataset`; \n",
    "\n",
    "For datasets with difficulty annotations, samples are stratified using a 1:3 Easy-to-Hard ratio. Creative tasks, which lack explicit difficulty labels, are included as a fixed-size subset selected based on preference-related signals derived from user feedback.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>\n",
    "        <img src=\"https://raw.githubusercontent.com/san9min/tunix-hack/main/assets/rl_dataset_level_distribution.webp\" width=\"45%\">\n",
    "      </td>\n",
    "      <td>\n",
    "        <img src=\"https://raw.githubusercontent.com/san9min/tunix-hack/main/assets/rl_dataset_domain_distribution.webp\" width=\"45%\">\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "\n",
    "You can use here:\n",
    "[DOD Gemma2 RL Dataset](https://www.kaggle.com/datasets/sangminlee09/small-reasoning-model-rl-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Finetuning code\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q kagglehub\n",
    "\n",
    "!pip install -q ipywidgets\n",
    "\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "!pip install \"google-tunix[prod]==0.1.3\"\n",
    "\n",
    "!pip uninstall -q -y flax\n",
    "# !pip install -U flax\n",
    "!pip install flax==0.12.0\n",
    "\n",
    "!pip install -q datasets wandb==0.22.0\n",
    "!pip install -q 'numpy>2'\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# =========================\n",
    "# Standard library\n",
    "# =========================\n",
    "import csv\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from fractions import Fraction\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Sequence, Tuple, Callable\n",
    "\n",
    "# =========================\n",
    "# Third-party libraries\n",
    "# =========================\n",
    "import grain.python as grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "import wandb\n",
    "from flax import nnx\n",
    "from orbax import checkpoint as ocp\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "# =========================\n",
    "# Tunix / Project-specific\n",
    "# =========================\n",
    "from tunix import PeftTrainer, TrainingConfig  \n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma import model as model_lib\n",
    "from tunix.models.gemma import params as params_lib\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import peft_trainer, utils\n",
    "from tunix.sft.metrics_logger import MetricsLogger \n",
    "from tunix.sft import metrics_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ================ Path & Checkpoints =================\n",
    "# =====================================================\n",
    "\n",
    "# TensorBoard logs for SFT training\n",
    "SFT_LOG_DIR = \"/tmp/content/tmp/tensorboard/sft\"\n",
    "SFT_CKPT_DIR = \"/kaggle/working/sft_ckpts/actor\"\n",
    "\n",
    "# Intermediate checkpoints (for recovery / resume)\n",
    "SFT_INTERMEDIATE_CKPT_DIR = \"/kaggle/working/ckpts/intermediate_ckpt\"\n",
    "\n",
    "# SFT stage 2 checkpoints\n",
    "SFT_STAGE2_CKPT_DIR = \"/kaggle/working/sft_ckpts/actor2\"\n",
    "\n",
    "# Final checkpoints after RL\n",
    "FINAL_CKPT_DIR = \"/kaggle/working/ckpts\"\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ======================= Data ========================\n",
    "# =====================================================\n",
    "\n",
    "# SFT stage 1 dataset (full general-domain reasoning data)\n",
    "SFT_TRAIN_DATA_DIR = (\n",
    "    \"/kaggle/input/small-reasoning-model-sft-dataset/\"\n",
    "    \"small_reasoning_model_sft_dataset.csv\"\n",
    ")\n",
    "\n",
    "# SFT stage 2 dataset (shorter reasoning, math-heavy subset)\n",
    "SFT_STAGE2_TRAIN_DATA_DIR = (\n",
    "    \"/kaggle/input/small-reasoning-model-sft-subset/\"\n",
    "    \"small_reasoning_model_sft_dataset_stage2.csv\"\n",
    ")\n",
    "\n",
    "# RL training dataset (verifiable + non-verifiable tasks)\n",
    "RL_TRAIN_DATA_DIR = (\n",
    "    \"/kaggle/input/small-reasoning-model-rl-dataset/\"\n",
    "    \"small_reasoning_model_rl_dataset.csv\"\n",
    ")\n",
    "\n",
    "# Fraction of dataset to use (1.0 = full dataset)\n",
    "SFT_TRAIN_FRACTION = 1.0\n",
    "RL_TRAIN_FRACTION = 1.0\n",
    "\n",
    "# ======================= LoRA ========================\n",
    "\n",
    "# LoRA rank (controls adapter capacity)\n",
    "RANK = 32\n",
    "\n",
    "# LoRA scaling factor\n",
    "ALPHA = 64.0\n",
    "\n",
    "# ===================== Sharding ======================\n",
    "# Device mesh configuration\n",
    "# (fsdp: parameter sharding, tp: tensor parallelism)\n",
    "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
    "\n",
    "# =================== SFT Stage 1 =====================\n",
    "\n",
    "# Maximum training steps\n",
    "#  - 0  : skip SFT entirely\n",
    "#  - -1 : automatically computed from dataset size\n",
    "SFT_MAX_STEPS = -1\n",
    "\n",
    "# Per-device micro batch size\n",
    "SFT_TRAIN_MICRO_BATCH_SIZE = 2\n",
    "\n",
    "# Gradient accumulation steps\n",
    "SFT_GRAD_ACCUM_STEPS = 64\n",
    "\n",
    "# Number of epochs\n",
    "SFT_NUM_EPOCHS = 1\n",
    "\n",
    "# Evaluation frequency (in steps)\n",
    "SFT_EVAL_EVERY_N_STEPS = 150\n",
    "\n",
    "# Base learning rate\n",
    "SFT_LEARNING_RATE = 5e-5\n",
    "\n",
    "# Warmup steps for LR scheduler\n",
    "SFT_WARMUP_STEPS = 20\n",
    "\n",
    "# Weight decay for optimizer\n",
    "SFT_WEIGHT_DECAY = 0.1\n",
    "\n",
    "# Maximum sequence length\n",
    "SFT_MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# Checkpoint save frequency\n",
    "SFT_SAVE_INTERVAL_STEPS = 100\n",
    "\n",
    "# Maximum number of checkpoints to keep\n",
    "SFT_MAX_TO_KEEP = 5\n",
    "\n",
    "# Gradient clipping norm\n",
    "SFT_MAX_GRAD_NORM = 0.1\n",
    "\n",
    "# =================== SFT Stage 2 =====================\n",
    "\n",
    "# Lower learning rate for refinement stage\n",
    "SFT_STAGE2_LEARNING_RATE = 1e-5\n",
    "\n",
    "# Number of epochs for stage 2\n",
    "SFT_STAGE2_NUM_EPOCHS = 1\n",
    "\n",
    "# Shorter max sequence length (target reasoning length)\n",
    "SFT_STAGE2_MAX_SEQ_LENGTH = 1280\n",
    "\n",
    "# Maximum training steps (-1 = auto)\n",
    "SFT_STAGE2_MAX_STEPS = -1\n",
    "\n",
    "# Checkpoint save frequency\n",
    "SFT_STAGE2_SAVE_INTERVAL_STEPS = 4\n",
    "\n",
    "# ==================== RL (GRPO) ======================\n",
    "\n",
    "# Maximum prompt length fed to the policy\n",
    "RL_MAX_PROMPT_LENGTH = 256\n",
    "\n",
    "# Total generation length\n",
    "RL_TOTAL_GENERATION_STEPS = 768\n",
    "\n",
    "# Sampling temperature (keep relatively high for diversity)\n",
    "RL_TEMPERATURE = 0.6\n",
    "\n",
    "# Nucleus sampling\n",
    "RL_TOP_P = 1.0\n",
    "\n",
    "# Top-k sampling\n",
    "RL_TOP_K = 50\n",
    "\n",
    "# Number of responses generated per prompt (G in GRPO)\n",
    "RL_NUM_GENERATIONS = 4\n",
    "\n",
    "# Number of GRPO inner iterations (Œº in the paper)\n",
    "RL_NUM_ITERATIONS = 1\n",
    "\n",
    "# KL divergence penalty coefficient (Œ≤)\n",
    "BETA = 0.08\n",
    "\n",
    "# PPO-style clipping epsilon (Œµ)\n",
    "EPSILON = 0.2\n",
    "\n",
    "# Per-device micro batch size\n",
    "RL_TRAIN_MICRO_BATCH_SIZE = 2\n",
    "\n",
    "# Number of batches per epoch\n",
    "RL_NUM_BATCHES = 1200\n",
    "\n",
    "# Number of evaluation batches\n",
    "RL_NUM_TEST_BATCHES = 100\n",
    "\n",
    "# Evaluation frequency\n",
    "RL_EVAL_EVERY_N_STEPS = 100\n",
    "\n",
    "# Number of epochs\n",
    "RL_NUM_EPOCHS = 1\n",
    "\n",
    "# Total number of RL training steps\n",
    "RL_MAX_STEPS = int(\n",
    "    RL_NUM_BATCHES * RL_NUM_ITERATIONS * RL_TRAIN_FRACTION * RL_NUM_EPOCHS\n",
    ")\n",
    "\n",
    "# Optimizer learning rate\n",
    "RL_LEARNING_RATE = 1e-6\n",
    "\n",
    "# Adam optimizer beta parameters\n",
    "RL_B1 = 0.9\n",
    "RL_B2 = 0.99\n",
    "\n",
    "# Weight decay\n",
    "RL_WEIGHT_DECAY = 0.1\n",
    "\n",
    "# Warmup steps (fraction of total steps)\n",
    "RL_WARMUP_STEPS = 0.1 * RL_MAX_STEPS\n",
    "\n",
    "# Gradient clipping norm\n",
    "RL_MAX_GRAD_NORM = 0.1\n",
    "\n",
    "# Checkpoint save frequency\n",
    "RL_SAVE_INTERVAL_STEPS = 200\n",
    "\n",
    "# Maximum number of checkpoints to keep\n",
    "RL_MAX_TO_KEEP = 8\n",
    "\n",
    "# ============================================================== #\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "\n",
    "# Use these standard output tags so that your model's output follow this format in plain text (no JSON/XML):\n",
    "# <reasoning>model_reasoning_trace</reasoning>\n",
    "# <answer>model_final_answer</answer>\n",
    "\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "SOLUTION_START = \"<answer>\"\n",
    "SOLUTION_END = \"</answer>\"\n",
    "\n",
    "# Use these parameters for greedy decoding; used in competition evaluation\n",
    "INF_TEMPERATURE=None\n",
    "INF_TOP_K=1\n",
    "INF_TOP_P=None\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt & Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same system prompt and template for both the SFT and RL stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"You are helpful Assistant. The user asks a question, and you solve it.\n",
    "You first thinks about the reasoning process in the mind and then provides the user\n",
    "with the answer. The reasoning process and answer are enclosed within {REASONING_START} {REASONING_END} and\n",
    "{SOLUTION_START} {SOLUTION_END} tags, respectively, i.e., {REASONING_START} reasoning process here {REASONING_END} {SOLUTION_START} answer here {SOLUTION_END}.\n",
    "\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "{system_prompt}\n",
    "User: {user}. Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Data Preprocessing\n",
    "\n",
    "We preprocess the SFT data by tokenizing each prompt‚Äìcompletion pair with a fixed maximum sequence length. The prompt tokens are masked out from the loss, and supervision is applied only to the completion tokens. When sequences exceed the maximum length, the prompt is truncated first to preserve as much of the completion (CoT Process) as possible. All samples are padded to a uniform length to enable efficient batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_tokenizer():\n",
    "    kaggle_ckpt_path = kagglehub.model_download(\n",
    "        \"google/gemma-2/flax/gemma2-2b-it\"\n",
    "      )\n",
    "    tokenizer = tokenizer_lib.Tokenizer(\n",
    "          tokenizer_path=os.path.join(kaggle_ckpt_path, \"tokenizer.model\"))\n",
    "    return tokenizer, kaggle_ckpt_path\n",
    "\n",
    "\n",
    "\n",
    "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
    "    pad_mask = x.input_tokens != tokenizer.pad_id()\n",
    "    positions = utils.build_positions_from_mask(pad_mask)\n",
    "    attention_mask = utils.make_causal_attn_mask(pad_mask)\n",
    "\n",
    "    return {\n",
    "        \"input_tokens\": x.input_tokens,\n",
    "        \"input_mask\": x.input_mask,          # <-- LOSS MASK (completion-only)\n",
    "        \"positions\": positions,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import grain.python as grain\n",
    "\n",
    "TrainingInput = peft_trainer.TrainingInput\n",
    "\n",
    "# =========================================\n",
    "# 1) Tokenize: (prompt_tokens, completion_tokens)\n",
    "# =========================================\n",
    "class _TokenizeSFT(grain.MapTransform):\n",
    "    def __init__(self, tokenizer, *, template: str, system_prompt: str,):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._template = template\n",
    "        self._system_prompt = system_prompt\n",
    "\n",
    "    def map(self, element: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        def _as_text(v):\n",
    "            return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
    "\n",
    "        user_text = _as_text(element[\"user\"])\n",
    "        assistant_text = _as_text(element[\"assistant\"])\n",
    "\n",
    "        prompt = self._template.format(\n",
    "            system_prompt=self._system_prompt,\n",
    "            user=user_text,\n",
    "        )\n",
    "        completion = assistant_text\n",
    "\n",
    "        prompt_tokens = np.asarray(list(self._tokenizer.encode(prompt)), dtype=np.int32)\n",
    "        comp_tokens   = np.asarray(list(self._tokenizer.encode(completion)), dtype=np.int32)\n",
    "\n",
    "        return prompt_tokens, comp_tokens\n",
    "\n",
    "# =========================================\n",
    "# 2) BuildTrainInput: concat + completion-only mask + (TRUNCATE + pad)\n",
    "# =========================================\n",
    "class _BuildTrainInputSFT(grain.MapTransform):\n",
    "    def __init__(self, max_seq_len: int, pad_id: int, eos_id: int):\n",
    "        self._max_seq_len = int(max_seq_len)\n",
    "        self._pad_id = int(pad_id)\n",
    "        self._eos_id = int(eos_id)\n",
    "\n",
    "    def map(self, tokens: Tuple[np.ndarray, np.ndarray]) -> TrainingInput:\n",
    "        prompt_tokens, comp_tokens = tokens\n",
    "    \n",
    "        # -------------------------\n",
    "        # 1) Ensure EOS at end of completion (same intent as tokenize_sft_for_tunix)\n",
    "        # -------------------------\n",
    "        if self._eos_id is not None:\n",
    "            if comp_tokens.size == 0 or comp_tokens[-1] != self._eos_id:\n",
    "                comp_tokens = np.concatenate(\n",
    "                    [comp_tokens.astype(np.int32, copy=False), np.asarray([self._eos_id], np.int32)],\n",
    "                    axis=0,\n",
    "                )\n",
    "    \n",
    "        # -------------------------\n",
    "        # 2) Concat + completion-only loss mask\n",
    "        # -------------------------\n",
    "        prompt_tokens = prompt_tokens.astype(np.int32, copy=False)\n",
    "        comp_tokens = comp_tokens.astype(np.int32, copy=False)\n",
    "    \n",
    "        input_tokens = np.concatenate([prompt_tokens, comp_tokens], axis=0)\n",
    "    \n",
    "        q_mask = np.zeros(prompt_tokens.shape[0], dtype=np.bool_)\n",
    "        a_mask = np.ones(comp_tokens.shape[0], dtype=np.bool_)\n",
    "        input_mask = np.concatenate([q_mask, a_mask], axis=0)\n",
    "    \n",
    "        # -------------------------\n",
    "        # 3) Truncate (keep completion as much as possible by trimming prompt first)\n",
    "        # -------------------------\n",
    "        max_len = self._max_seq_len\n",
    "        if input_tokens.shape[0] > max_len:\n",
    "            overflow = int(input_tokens.shape[0] - max_len)\n",
    "    \n",
    "            # (a) drop from the LEFT of the prompt first\n",
    "            drop = min(overflow, int(prompt_tokens.shape[0]))\n",
    "            if drop > 0:\n",
    "                input_tokens = input_tokens[drop:]\n",
    "                input_mask = input_mask[drop:]\n",
    "                overflow -= drop\n",
    "    \n",
    "            # (b) if still overflow, drop from the RIGHT (may cut completion tail)\n",
    "            if overflow > 0:\n",
    "                input_tokens = input_tokens[:-overflow]\n",
    "                input_mask = input_mask[:-overflow]\n",
    "    \n",
    "        # -------------------------\n",
    "        # 4) Pad up to max_len (mask pad = 0)\n",
    "        # -------------------------\n",
    "        input_tokens = self._pad_up_to_max_len(input_tokens, self._pad_id)\n",
    "        input_mask   = self._pad_up_to_max_len(input_mask, 0)\n",
    "    \n",
    "        return TrainingInput(input_tokens=input_tokens, input_mask=input_mask)\n",
    "\n",
    "\n",
    "    def _pad_up_to_max_len(self, arr: np.ndarray, pad_value: int) -> np.ndarray:\n",
    "        seq_len = arr.shape[0]\n",
    "        to_pad = max(self._max_seq_len - seq_len, 0)\n",
    "        return np.pad(arr, [[0, to_pad]], mode=\"constant\", constant_values=pad_value)\n",
    "\n",
    "# =========================================\n",
    "# 3) DataLoader builder\n",
    "# =========================================\n",
    "def build_sft_dataloader(\n",
    "    *,\n",
    "    data_source: grain.RandomAccessDataSource,\n",
    "    batch_size: int,\n",
    "    num_epochs: int | None,\n",
    "    max_seq_len: int,\n",
    "    tokenizer,\n",
    "    template: str,\n",
    "    system_prompt: str,\n",
    "    drop_remainder: bool = True,\n",
    ") -> grain.DataLoader:\n",
    "    return grain.DataLoader(\n",
    "        data_source=data_source,\n",
    "        sampler=grain.IndexSampler(\n",
    "            num_records=len(data_source),\n",
    "            num_epochs=num_epochs,\n",
    "            shard_options=grain.NoSharding(),\n",
    "        ),\n",
    "        operations=[\n",
    "            _TokenizeSFT(tokenizer, template=template, system_prompt=system_prompt),\n",
    "            _BuildTrainInputSFT(max_seq_len, pad_id=int(tokenizer.pad_id()), eos_id=int(tokenizer.eos_id())),\n",
    "            grain.Batch(batch_size=batch_size, drop_remainder=True),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "def create_sft_dataset(\n",
    "    df_sft,\n",
    "    *,\n",
    "    tokenizer,\n",
    "    template: str,\n",
    "    system_prompt: str,\n",
    "    train_micro_batch_size: int,\n",
    "    eval_micro_batch_size: int,\n",
    "    num_train_epochs: int | None,\n",
    "    max_seq_len: int,\n",
    "    train_fraction: float,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    if not (0.0 < train_fraction <= 1.0):\n",
    "        raise ValueError(\"train_fraction must be in (0, 1].\")\n",
    "\n",
    "    records = df_sft.to_dict(orient=\"records\")\n",
    "    n = len(records)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"df_sft is empty.\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(n)\n",
    "\n",
    "    # -------------------------\n",
    "    # split\n",
    "    # -------------------------\n",
    "    if train_fraction == 1.0:\n",
    "        train_idx = perm\n",
    "        val_idx = None\n",
    "    else:\n",
    "        n_train = int(np.floor(n * train_fraction))\n",
    "        n_train = max(1, n_train)\n",
    "        train_idx = perm[:n_train]\n",
    "        val_idx = perm[n_train:]\n",
    "\n",
    "    train_records = [records[i] for i in train_idx]\n",
    "    val_records = [] if (val_idx is None) else [records[i] for i in val_idx]\n",
    "\n",
    "\n",
    "    # -------------------------\n",
    "    # build loaders (real)\n",
    "    # -------------------------\n",
    "    train_ds = grain.MapDataset.source(train_records)\n",
    "    sft_train_loader = build_sft_dataloader(\n",
    "        data_source=train_ds,\n",
    "        batch_size=train_micro_batch_size,\n",
    "        num_epochs=num_train_epochs,\n",
    "        max_seq_len=max_seq_len,\n",
    "        tokenizer=tokenizer,\n",
    "        template=template,\n",
    "        system_prompt=system_prompt,\n",
    "        drop_remainder=True,  # Í∏∞Ï°¥Í≥º ÎèôÏùº\n",
    "    )\n",
    "\n",
    "    if val_idx is None or len(val_records) == 0:\n",
    "        sft_val_loader = None\n",
    "    else:\n",
    "        val_ds = grain.MapDataset.source(val_records)\n",
    "        sft_val_loader = build_sft_dataloader(\n",
    "            data_source=val_ds,\n",
    "            batch_size=eval_micro_batch_size,\n",
    "            num_epochs=1,\n",
    "            max_seq_len=max_seq_len,\n",
    "            tokenizer=tokenizer,\n",
    "            template=template,\n",
    "            system_prompt=system_prompt,\n",
    "            drop_remainder=True,\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # summary print\n",
    "    # -------------------------\n",
    "    raw_train = len(train_records)\n",
    "    raw_val = 0 if val_idx is None else len(val_records)\n",
    "\n",
    "\n",
    "    print(\n",
    "            f\"[split] total={n:,}  train={raw_train:,}  \"\n",
    "            f\"val={raw_val:,}  train_fraction={train_fraction}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    return sft_train_loader, sft_val_loader, raw_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Tokenizer\n",
    "\n",
    "# Initialize tokenizer and load base model checkpoint\n",
    "# - tokenizer: used for SFT/RL preprocessing and generation\n",
    "# - kaggle_ckpt_path: local path to the downloaded base model\n",
    "tokenizer, kaggle_ckpt_path = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load SFT Datasets \n",
    "\n",
    "# Load SFT stage 1 dataset\n",
    "# - General-domain reasoning data\n",
    "# - Used to stabilize overall reasoning structure with longer CoT\n",
    "df_sft = pd.read_csv(SFT_TRAIN_DATA_DIR)\n",
    "\n",
    "sft_train_dataset, sft_val_dataset, effective_train_len = create_sft_dataset(\n",
    "    df_sft,\n",
    "    tokenizer=tokenizer,\n",
    "    template=TEMPLATE,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    train_micro_batch_size=SFT_TRAIN_MICRO_BATCH_SIZE,\n",
    "    eval_micro_batch_size=SFT_TRAIN_MICRO_BATCH_SIZE,\n",
    "    num_train_epochs=SFT_NUM_EPOCHS,\n",
    "    max_seq_len=SFT_MAX_SEQ_LENGTH,\n",
    "    train_fraction=SFT_TRAIN_FRACTION,\n",
    "    seed=SEED,   \n",
    " )\n",
    "\n",
    "# Load SFT stage 2 dataset\n",
    "# - Shorter reasoning sequences with higher math proportion (Subset of stage 1)\n",
    "# - Used for refinement toward target reasoning length\n",
    "df_sft_stage2 = pd.read_csv(SFT_STAGE2_TRAIN_DATA_DIR)\n",
    "\n",
    "sft_stage2_train_dataset, sft_stage2_val_dataset, stage2_effective_train_len = create_sft_dataset(\n",
    "    df_sft_stage2,\n",
    "    tokenizer=tokenizer,\n",
    "    template=TEMPLATE,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    train_micro_batch_size=SFT_TRAIN_MICRO_BATCH_SIZE,\n",
    "    eval_micro_batch_size=SFT_TRAIN_MICRO_BATCH_SIZE,\n",
    "    num_train_epochs=SFT_STAGE2_NUM_EPOCHS,\n",
    "    max_seq_len=SFT_STAGE2_MAX_SEQ_LENGTH,\n",
    "    train_fraction=SFT_TRAIN_FRACTION,\n",
    "    seed=SEED,   \n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Auto-compute SFT stage 1 steps and checkpoint interval\n",
    "if SFT_MAX_STEPS == -1:\n",
    "    SFT_MAX_STEPS = (\n",
    "        effective_train_len * SFT_NUM_EPOCHS //\n",
    "        (SFT_GRAD_ACCUM_STEPS * SFT_TRAIN_MICRO_BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "# Auto-compute SFT stage 2 steps and checkpoint interval\n",
    "if SFT_STAGE2_MAX_STEPS == -1:\n",
    "    SFT_STAGE2_MAX_STEPS = (\n",
    "        stage2_effective_train_len * SFT_STAGE2_NUM_EPOCHS //\n",
    "        (SFT_GRAD_ACCUM_STEPS * SFT_TRAIN_MICRO_BATCH_SIZE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Data Preprocessing\n",
    "\n",
    "We preprocess the RL dataset with a focus on domain-aware system prompt selection to support downstream reward function design. For each sample, the system prompt is chosen based on its domain: mathematical problems enforce a structured reasoning-and-answer format with the final output restricted to a numeric value, whereas non-mathematical tasks use a general-purpose prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Math\n",
    "MATH_SYSTEM_PROMPT =  f\"\"\"You are helpful assistant. You are given a math problem. Think about the problem and \\\n",
    "provide your reasoning. Place reasoning trace between {REASONING_START} and \\\n",
    "{REASONING_END}. Then, provide the only final answer (i.e., 'just' numerical value\\\n",
    ") between {SOLUTION_START} and {SOLUTION_END}.\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_BY_DOMAIN = {\n",
    "    \"math\": MATH_SYSTEM_PROMPT,\n",
    "    \"default\": SYSTEM_PROMPT,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _as_text(v) -> str:\n",
    "    \"\"\"Safely convert a value to string (handles None / NaN).\"\"\"\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(v):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return v if isinstance(v, str) else str(v)\n",
    "\n",
    "\n",
    "def _get(x: dict, k: str, default: str = \"\") -> str:\n",
    "    \"\"\"Dictionary getter with safe string conversion.\"\"\"\n",
    "    return _as_text(x.get(k, default))\n",
    "\n",
    "\n",
    "def create_rl_dataset(\n",
    "    df_rl: pd.DataFrame,\n",
    "    *,\n",
    "    shuffle_seed: int = 42,\n",
    ") -> grain.MapDataset:\n",
    "    \"\"\"\n",
    "    Build an RL training dataset for Tunix/Grain.\n",
    "\n",
    "    Expected columns:\n",
    "      - question (required)\n",
    "      - answer (required)\n",
    "      - answer_canon (optional): canonical/normalized answer; used if present\n",
    "      - reward_model_type (optional): used for reward routing/selection\n",
    "      - domain (optional): metadata for logging/routing\n",
    "      - answer_type (optional): metadata for logging/routing\n",
    "    \"\"\"\n",
    "    required_cols = {\"question\", \"answer\"}\n",
    "    missing = required_cols - set(df_rl.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}. Got={list(df_rl.columns)}\")\n",
    "\n",
    "    records = df_rl.to_dict(orient=\"records\")\n",
    "\n",
    "    def _map_record(x: dict) -> dict:\n",
    "        question = _as_text(x[\"question\"])\n",
    "\n",
    "        # Prefer canonical answer if provided; otherwise fall back to raw answer\n",
    "        answer_canon = _get(x, \"answer_canon\")\n",
    "        answer = answer_canon if answer_canon else _as_text(x[\"answer\"])\n",
    "        domain = _get(x, \"domain\").lower()\n",
    "\n",
    "        system_prompt = ( SYSTEM_PROMPT_BY_DOMAIN[\"math\"] if domain == \"math\" else SYSTEM_PROMPT_BY_DOMAIN[\"default\"])\n",
    "\n",
    "        return {\n",
    "            # Model input prompt\n",
    "            \"prompts\": TEMPLATE.format(\n",
    "            system_prompt=system_prompt,\n",
    "            user=question,\n",
    "            ),\n",
    "\n",
    "            # Raw fields for reward function\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "\n",
    "            # Reward routing / metadata (optional)\n",
    "            \"reward_model_type\": _get(x, \"reward_model_type\"),\n",
    "            \"domain\": _get(x, \"domain\"),\n",
    "            \"answer_type\": _get(x, \"answer_type\"),\n",
    "        }\n",
    "\n",
    "    ds = (\n",
    "        grain.MapDataset.source(records)\n",
    "        .shuffle(seed=shuffle_seed)\n",
    "        .map(_map_record)\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load + Pre-filter\n",
    "# Drop samples whose question exceeds the maximum token budget (Anti-OOM)\n",
    "# ============================================================\n",
    "\n",
    "df_rl = pd.read_csv(RL_TRAIN_DATA_DIR)\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens using the global `tokenizer`.\"\"\"\n",
    "    text = _as_text(text)\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "before_n = len(df_rl)\n",
    "q_tok_lens = df_rl[\"question\"].apply(count_tokens)\n",
    "df_rl = df_rl[q_tok_lens <= RL_MAX_PROMPT_LENGTH].reset_index(drop=True)\n",
    "after_n = len(df_rl)\n",
    "\n",
    "print(f\"[filter] question tokens <= {RL_MAX_PROMPT_LENGTH}: {before_n} -> {after_n}\")\n",
    "\n",
    "# ============================================================\n",
    "# Build dataset\n",
    "# ============================================================\n",
    "\n",
    "rl_dataset = create_rl_dataset(df_rl).batch(RL_TRAIN_MICRO_BATCH_SIZE)[:RL_NUM_BATCHES]\n",
    "\n",
    "\n",
    "if RL_TRAIN_FRACTION == 1.0:\n",
    "    rl_train_dataset = rl_dataset.repeat(RL_NUM_EPOCHS)\n",
    "    rl_val_dataset = None\n",
    "else:\n",
    "    n_total_rl = len(rl_dataset)\n",
    "    n_train_rl = int(n_total_rl * RL_TRAIN_FRACTION)\n",
    "\n",
    "    rl_train_dataset = rl_dataset[:n_train_rl].repeat(RL_NUM_EPOCHS)\n",
    "    rl_val_dataset = rl_dataset[n_train_rl:].repeat(RL_NUM_EPOCHS)\n",
    "\n",
    "rl_dataset_lengths = (\n",
    "    len(rl_train_dataset),\n",
    "    len(rl_val_dataset) if rl_val_dataset is not None else 0,\n",
    ")\n",
    "print(f\"dataset contains {rl_dataset_lengths} of batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for ele in rl_train_dataset[:1]:\n",
    "#   pprint(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Reward Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Weights\n",
    "# =========================\n",
    "@dataclass\n",
    "class RuleWeights:\n",
    "    # Format-related rewards and penalties\n",
    "    format_exact: float = 3.0\n",
    "    format_missing_penalty: float = -1.0\n",
    "    format_order_penalty: float = -0.5  # Less harsh penalty to preserve exploration in early training\n",
    "    format_empty_reasoning_penalty: float = -0.5\n",
    "    format_empty_answer_penalty: float = -0.5\n",
    "\n",
    "    # Answer-related rewards (string and numeric matching)\n",
    "    answer_exact: float = 3.0\n",
    "    answer_strip_match: float = 1.5\n",
    "    answer_ratio_0p9_1p1: float = 0.5\n",
    "    answer_ratio_0p8_1p2: float = 0.25\n",
    "    answer_wrong_penalty: float = -1.0\n",
    "\n",
    "    # Parse-failure penalty is recommended to be 0 to avoid over-penalizing sentence-style or non-structured tasks\n",
    "    answer_parse_fail_penalty: float = 0.0\n",
    "\n",
    "    # Reward for cases where the sentence contains a matching number (check_numbers-style)\n",
    "    numbers_exact: float = 1.5\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Parsing helpers\n",
    "# =========================\n",
    "_FRAC_LATEX = re.compile(\n",
    "    r\"\"\"\\\\(?:frac|dfrac|tfrac)\\s*\n",
    "        \\{\\s*([+-]?\\d+)\\s*\\}\\s*\n",
    "        \\{\\s*([+-]?\\d+)\\s*\\}\n",
    "    \"\"\",\n",
    "    re.VERBOSE,\n",
    ")\n",
    "_FRAC_SLASH = re.compile(r\"^\\s*([+-]?\\d+)\\s*/\\s*([+-]?\\d+)\\s*$\")\n",
    "_PERCENT = re.compile(r\"^\\s*([+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+))\\s*%\\s*$\")\n",
    "_FLOATLIKE = re.compile(r\"^\\s*([+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][+-]?\\d+)?)\\s*$\")\n",
    "_NUM_ANYWHERE = re.compile(r\"([+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][+-]?\\d+)?)\")\n",
    "\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.replace(\"$\", \"\").replace(\",\", \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _parse_number(s: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - Fraction for exact rational forms (latex frac / a/b / percent / integer)\n",
    "      - float for float-like forms\n",
    "      - None if cannot parse\n",
    "    \"\"\"\n",
    "    s = _normalize_text(s)\n",
    "\n",
    "    # LaTeX-style fraction appearing anywhere in the string\n",
    "    m = _FRAC_LATEX.search(s)\n",
    "    if m:\n",
    "        num = int(m.group(1))\n",
    "        den = int(m.group(2))\n",
    "        if den == 0:\n",
    "            return None\n",
    "        return Fraction(num, den)\n",
    "\n",
    "    # Pure slash-style fraction (a/b)\n",
    "    m = _FRAC_SLASH.match(s)\n",
    "    if m:\n",
    "        num = int(m.group(1))\n",
    "        den = int(m.group(2))\n",
    "        if den == 0:\n",
    "            return None\n",
    "        return Fraction(num, den)\n",
    "\n",
    "    # Percentage form (exact)\n",
    "    m = _PERCENT.match(s)\n",
    "    if m:\n",
    "        val = Fraction(m.group(1))\n",
    "        return val / 100\n",
    "\n",
    "    # Integer form (exact)\n",
    "    if re.fullmatch(r\"[+-]?\\d+\", s):\n",
    "        return Fraction(int(s), 1)\n",
    "\n",
    "    # Floating-point or scientific notation\n",
    "    m = _FLOATLIKE.match(s)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def _extract_last_number_token(s: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the last numeric expression from a string.\n",
    "    Priority (from strongest to weakest):\n",
    "      1) LaTeX fraction  \\\\frac{a}{b}\n",
    "      2) Slash fraction  a/b\n",
    "      3) Percent         12%\n",
    "      4) Float / integer 0.34, 5, 1e-3\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "\n",
    "    s = _normalize_text(s)\n",
    "\n",
    "    # 1) LaTeX fraction (use the last occurrence)\n",
    "    latex_fracs = list(_FRAC_LATEX.finditer(s))\n",
    "    if latex_fracs:\n",
    "        m = latex_fracs[-1]\n",
    "        return m.group(0)\n",
    "\n",
    "    # 2) Slash-style fraction a/b\n",
    "    slash_fracs = list(_FRAC_SLASH.finditer(s))\n",
    "    if slash_fracs:\n",
    "        m = slash_fracs[-1]\n",
    "        return m.group(0)\n",
    "\n",
    "    # 3) Percentage expression\n",
    "    percents = list(_PERCENT.finditer(s))\n",
    "    if percents:\n",
    "        return percents[-1].group(0)\n",
    "\n",
    "    # 4) Floating-point or integer number\n",
    "    nums = _NUM_ANYWHERE.findall(s)\n",
    "    if nums:\n",
    "        return nums[-1]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _ratio_score(w: RuleWeights, ratio: float) -> float:\n",
    "    if 0.9 <= ratio <= 1.1:\n",
    "        return w.answer_ratio_0p9_1p1\n",
    "    if 0.8 <= ratio <= 1.2:\n",
    "        return w.answer_ratio_0p8_1p2\n",
    "    return w.answer_wrong_penalty\n",
    "\n",
    "\n",
    "def _to_float(x) -> float:\n",
    "    return float(x) if not isinstance(x, Fraction) else float(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Reward\n",
    "# =========================\n",
    "class RuleBasedRewardFn:\n",
    "    \"\"\"\n",
    "    Expected strict response format:\n",
    "      <reasoning> ... </reasoning>\n",
    "      <answer> ... </answer>\n",
    "\n",
    "    This implementation focuses on:\n",
    "      - format_exact\n",
    "      - format_exact+answer (single pathway, numeric-aware)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reasoning_start: str = \"<reasoning>\",\n",
    "        reasoning_end: str = \"</reasoning>\",\n",
    "        solution_start: str = \"<answer>\",\n",
    "        solution_end: str = \"</answer>\",\n",
    "        weights: Optional[RuleWeights] = None,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.reasoning_start = reasoning_start\n",
    "        self.reasoning_end = reasoning_end\n",
    "        self.solution_start = solution_start\n",
    "        self.solution_end = solution_end\n",
    "        self.weights = weights or RuleWeights()\n",
    "        self.debug = debug\n",
    "\n",
    "        # Strict full-match: no extra text allowed outside the tags\n",
    "        self.match_format_strict = re.compile(\n",
    "            rf\"^[\\s]*\"\n",
    "            rf\"{re.escape(self.reasoning_start)}(?P<reasoning>.+?){re.escape(self.reasoning_end)}\"\n",
    "            rf\"[\\s]*\"\n",
    "            rf\"{re.escape(self.solution_start)}(?P<answer>.+?){re.escape(self.solution_end)}\"\n",
    "            rf\"[\\s]*$\",\n",
    "            flags=re.DOTALL,\n",
    "        )\n",
    "\n",
    "    # ------------------------\n",
    "    # Format reward\n",
    "    # ------------------------\n",
    "    def format_exact(self, prompts, completions, **kwargs) -> List[float]:\n",
    "        w = self.weights\n",
    "        scores: List[float] = []\n",
    "\n",
    "        rs, re_ = self.reasoning_start, self.reasoning_end\n",
    "        as_, ae = self.solution_start, self.solution_end\n",
    "\n",
    "        for response in completions:\n",
    "            m = self.match_format_strict.search(response)\n",
    "            if m is not None:\n",
    "                reasoning = _normalize_text(m.group(\"reasoning\"))\n",
    "                ans = _normalize_text(m.group(\"answer\"))\n",
    "\n",
    "                if reasoning.strip() == \"\":\n",
    "                    scores.append(w.format_empty_reasoning_penalty)\n",
    "                    continue\n",
    "                if ans.strip() == \"\":\n",
    "                    scores.append(w.format_empty_answer_penalty)\n",
    "                    continue\n",
    "\n",
    "                scores.append(w.format_exact)\n",
    "                continue\n",
    "\n",
    "            # Non-strict format ‚Üí apply negative rewards\n",
    "            has_all = (rs in response) and (re_ in response) and (as_ in response) and (ae in response)\n",
    "            if not has_all:\n",
    "                scores.append(w.format_missing_penalty)\n",
    "                continue\n",
    "\n",
    "            # All tags exist but strict match fails (wrong order, duplication, nesting, or trailing text)\n",
    "            rpos = response.find(rs)\n",
    "            apos = response.find(as_)\n",
    "            if rpos != -1 and apos != -1 and rpos > apos:\n",
    "                scores.append(w.format_order_penalty)\n",
    "            else:\n",
    "                scores.append(w.format_order_penalty)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    # ------------------------\n",
    "    # Answer scoring (numeric-aware)\n",
    "    # ------------------------\n",
    "    def _answer_score(self, guess: str, true: str) -> float:\n",
    "        \"\"\"\n",
    "          1) exact / strip match\n",
    "          2) if both parse as numbers: exact or ratio reward\n",
    "          3) else extract first number from guess and compare to numeric true (exact -> numbers_exact else 0)\n",
    "        \"\"\"\n",
    "        w = self.weights\n",
    "        guess = _normalize_text(guess)\n",
    "        true = _normalize_text(true)\n",
    "\n",
    "        # (1) String-level exact and stripped matching\n",
    "        if guess == true:\n",
    "            return w.answer_exact\n",
    "        if guess.strip() == true.strip():\n",
    "            return w.answer_strip_match\n",
    "\n",
    "        # (2) Direct numeric parsing path\n",
    "        gnum = _parse_number(guess)\n",
    "        tnum = _parse_number(true)\n",
    "        if gnum is not None and tnum is not None:\n",
    "            # Exact fraction comparison case\n",
    "            if isinstance(gnum, Fraction) and isinstance(tnum, Fraction):\n",
    "                if gnum == tnum:\n",
    "                    return w.answer_exact\n",
    "                tv = float(tnum)\n",
    "                ratio = float(gnum) / tv if tv != 0.0 else math.inf\n",
    "                return _ratio_score(w, ratio)\n",
    "\n",
    "            # Floating-point comparison case\n",
    "            try:\n",
    "                gv = _to_float(gnum)\n",
    "                tv = _to_float(tnum)\n",
    "                if math.isclose(gv, tv, rel_tol=1e-6, abs_tol=1e-9):\n",
    "                    return w.answer_exact\n",
    "                ratio = gv / tv if tv != 0.0 else math.inf\n",
    "                return _ratio_score(w, ratio)\n",
    "            except Exception:\n",
    "                return w.answer_parse_fail_penalty\n",
    "        # (3) Extract the last numeric token from the guess and compare with the true value\n",
    "        tnum2 = _parse_number(true)\n",
    "        if tnum2 is None:\n",
    "            return w.answer_parse_fail_penalty\n",
    "\n",
    "        extracted = _extract_last_number_token(guess)\n",
    "        if extracted is None:\n",
    "            return w.answer_parse_fail_penalty\n",
    "\n",
    "        gnum2 = _parse_number(extracted)\n",
    "        if gnum2 is None:\n",
    "            return w.answer_parse_fail_penalty\n",
    "\n",
    "        try:\n",
    "            gv = _to_float(gnum2)\n",
    "            tv = _to_float(tnum2)\n",
    "            # check_numbers-style behavior: only exact matches receive partial reward\n",
    "            if math.isclose(gv, tv, rel_tol=1e-6, abs_tol=1e-12):\n",
    "                return w.numbers_exact\n",
    "\n",
    "            ratio = gv / tv if tv != 0.0 else math.inf\n",
    "            raw = _ratio_score(w, ratio)\n",
    "            return max(0.0, 0.5 * raw)\n",
    "\n",
    "        except Exception:\n",
    "            return w.answer_parse_fail_penalty\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompts,\n",
    "        completions,\n",
    "        answer=None,\n",
    "        rule_mode: str = \"format_exact\",\n",
    "        **kwargs,\n",
    "    ) -> List[float]:\n",
    "        rule_mode = rule_mode.strip()\n",
    "        w = self.weights\n",
    "\n",
    "        if rule_mode == \"format_exact\":\n",
    "            return self.format_exact(prompts, completions, **kwargs)\n",
    "\n",
    "        if rule_mode == \"format_exact+answer\":\n",
    "            if answer is None:\n",
    "                raise ValueError(\"rule_mode='format_exact+answer' requires `answer`.\")\n",
    "\n",
    "            fmt_scores = self.format_exact(prompts, completions, **kwargs)\n",
    "            out: List[float] = []\n",
    "\n",
    "            for response, true_raw, fscore in zip(completions, answer, fmt_scores):\n",
    "                # Gate: apply answer reward only when strict format is satisfied (to match stable, high-performing behavior)\n",
    "                if fscore != w.format_exact:\n",
    "                    out.append(fscore)\n",
    "                    continue\n",
    "\n",
    "                m = self.match_format_strict.search(response)\n",
    "                if m is None:\n",
    "                    out.append(fscore)\n",
    "                    continue\n",
    "\n",
    "                guess_raw = m.group(\"answer\")\n",
    "                ascore = self._answer_score(guess_raw, true_raw)\n",
    "                out.append(fscore + ascore)\n",
    "\n",
    "            return out\n",
    "\n",
    "        raise ValueError(f\"Unknown rule_mode: {rule_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_REASON_RE = re.compile(r\"<reasoning>(.*?)</reasoning>\", re.DOTALL)\n",
    "\n",
    "\n",
    "def _extract_reasoning(text: str) -> str:\n",
    "    m = _REASON_RE.search(text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "\n",
    "def _encode(tokenizer, s: str) -> List[int]:\n",
    "    return list(tokenizer.encode(s))\n",
    "\n",
    "\n",
    "def _left_pad_to(arr: np.ndarray, target_len: int, pad_id: int) -> np.ndarray:\n",
    "    if arr.shape[0] >= target_len:\n",
    "        return arr[-target_len:]\n",
    "    pad = np.full((target_len - arr.shape[0],), pad_id, dtype=np.int32)\n",
    "    return np.concatenate([pad, arr.astype(np.int32)], axis=0)\n",
    "\n",
    "\n",
    "def _right_pad_stack(token_lists: List[List[int]], pad_id: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns (tokens[B, L], mask[B, L]) where mask is 1 for non-pad.\"\"\"\n",
    "    maxlen = max((len(x) for x in token_lists), default=0)\n",
    "    toks = np.full((len(token_lists), maxlen), pad_id, dtype=np.int32)\n",
    "    msk = np.zeros((len(token_lists), maxlen), dtype=np.int32)\n",
    "    for i, ids in enumerate(token_lists):\n",
    "        if ids:\n",
    "            toks[i, : len(ids)] = np.asarray(ids, dtype=np.int32)\n",
    "            msk[i, : len(ids)] = 1\n",
    "    return toks, msk\n",
    "\n",
    "\n",
    "def _mean_prob_from_logps(logps: jnp.ndarray, mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    logps: [B, L] per-token log-probabilities (log p)\n",
    "    mask:  [B, L] 1 for valid tokens else 0\n",
    "    returns: [B] mean token *probability* (exp(log p)) over valid tokens\n",
    "        # Note: this is NOT mean log-prob; it averages exp(logp), which is typically small.\n",
    "    \"\"\"\n",
    "    mask_f = mask.astype(jnp.float32)\n",
    "    denom = jnp.clip(mask_f.sum(axis=-1), a_min=1.0)\n",
    "    probs = jnp.exp(logps)\n",
    "    return (probs * mask_f).sum(axis=-1) / denom\n",
    "\n",
    "\n",
    "def _apply_rlpr_std_prompt_filter(\n",
    "    rewards: np.ndarray,\n",
    "    *,\n",
    "    num_generations: int,\n",
    "    beta: float,\n",
    "    beta_min: float,\n",
    "    beta_max: float,\n",
    "    eps: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    rewards: [B] where B == N*G (G=num_generations)\n",
    "    Returns:\n",
    "      filtered_rewards: [B] (groups with std < beta are zeroed)\n",
    "      group_std:       [N] if filtering is applied; otherwise empty array\n",
    "    \"\"\"\n",
    "    G = int(num_generations)\n",
    "    if G <= 1 or rewards.shape[0] % G != 0:\n",
    "        return rewards, np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "    R = rewards.reshape(-1, G)  # [N, G]\n",
    "    sd = R.std(axis=1, ddof=1).astype(np.float32)\n",
    "    sd = np.maximum(sd, eps)\n",
    "\n",
    "    beta = float(np.clip(beta, beta_min, beta_max))\n",
    "    keep = sd >= beta  # [N]\n",
    "    R2 = R * keep[:, None].astype(R.dtype)  # zero-out low-std prompts\n",
    "    return R2.reshape(-1), sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StdFilterConfig:\n",
    "    enabled: bool = True\n",
    "    num_generations: int = 4\n",
    "    ema_decay: float = 0.99\n",
    "    beta_init: float = 0.0\n",
    "    beta_min: float = 0.0\n",
    "    beta_max: float = 1.0\n",
    "    eps: float = 1e-6\n",
    "    filtering_mode: Literal[\"fixed\", \"dynamic\"] = \"fixed\"\n",
    "\n",
    "\n",
    "class ProbabilityBasedRewardFn:\n",
    "    \"\"\"\n",
    "    RLPR-style Probability Reward using ACTOR (rollout policy) probs.\n",
    "\n",
    "    GRPO expects:\n",
    "      fn(prompts, completions, answer, **kwargs) -> List[float]\n",
    "\n",
    "    Requires training_input include:\n",
    "      - \"answer\" (ground truth y*)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        rl_cluster,\n",
    "        tokenizer,\n",
    "        max_prompt_length: Optional[int] = None,\n",
    "        micro_batch_size: Optional[int] = None,\n",
    "        use_debias: bool = True,\n",
    "        clip_min: float = 0.0,\n",
    "        clip_max: float = 1.0,\n",
    "        wrap_y_with_answer_tags: bool = True,\n",
    "        std_filter: Optional[StdFilterConfig] = None,\n",
    "        add_eos_to_y: bool = True,\n",
    "        name: str = \"probability_based_reward\",\n",
    "    \n",
    "    ):\n",
    "        self.rl_cluster = rl_cluster\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.__name__ = name\n",
    "\n",
    "        self.pad_id = int(self.rl_cluster.rollout.pad_id())\n",
    "        self.eos_id = int(self.rl_cluster.rollout.eos_id())\n",
    "\n",
    "\n",
    "        rc = self.rl_cluster.cluster_config.rollout_config\n",
    "        if max_prompt_length is None:\n",
    "            if isinstance(rc, dict):\n",
    "                # default: TRAIN config (dict key is Mode enum; safest is to pick TRAIN if exists)\n",
    "                train_key = None\n",
    "                for k in rc.keys():\n",
    "                    if str(k).lower() == \"train\" or getattr(k, \"value\", \"\").lower() == \"train\":\n",
    "                        train_key = k\n",
    "                        break\n",
    "                key = train_key if train_key is not None else next(iter(rc.keys()))\n",
    "                self.max_prompt_length = int(rc[key].max_prompt_length)\n",
    "            else:\n",
    "                self.max_prompt_length = int(rc.max_prompt_length)\n",
    "        else:\n",
    "            self.max_prompt_length = int(max_prompt_length)\n",
    "\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.use_debias = bool(use_debias)\n",
    "        self.clip_min = float(clip_min)\n",
    "        self.clip_max = float(clip_max)\n",
    "        self.wrap_y_with_answer_tags = bool(wrap_y_with_answer_tags)\n",
    "        self.std_filter = std_filter if std_filter is not None else StdFilterConfig(enabled=False)\n",
    "        self.add_eos_to_y = bool(add_eos_to_y)\n",
    "        self._beta = float(self.std_filter.beta_init)\n",
    "        self._beta_initialized = False\n",
    "\n",
    "    def _actor_per_token_logps(\n",
    "        self,\n",
    "        prompt_tok: jax.Array,\n",
    "        y_tok: jax.Array,\n",
    "        y_mask: jax.Array,\n",
    "    ) -> jax.Array:\n",
    "         # \"old\" logps = logps under the rollout policy used to sample trajectories\n",
    "        # (i.e., the current policy before the upcoming update step)\n",
    "        return self.rl_cluster.get_old_per_token_logps(\n",
    "            prompt_tokens=prompt_tok,\n",
    "            completion_tokens=y_tok,\n",
    "            micro_batch_size=self.micro_batch_size or int(y_tok.shape[0]),\n",
    "            completion_mask=y_mask,\n",
    "        )\n",
    "\n",
    "    def _get_current_beta(self, group_sd: Optional[np.ndarray]) -> float:\n",
    "        \"\"\"\n",
    "        Returns the beta to use this step.\n",
    "        - fixed: always beta_init\n",
    "        - dynamic: EMA over step_mean_sd (if group_sd provided and non-empty)\n",
    "        \"\"\"\n",
    "        current_filtering_mode = self.std_filter.filtering_mode\n",
    "    \n",
    "        if current_filtering_mode == \"fixed\":\n",
    "            beta = float(self.std_filter.beta_init)\n",
    "            beta = float(np.clip(beta, self.std_filter.beta_min, self.std_filter.beta_max))\n",
    "            self._beta = beta\n",
    "            self._beta_initialized = True\n",
    "            return beta\n",
    "    \n",
    "        if current_filtering_mode == \"dynamic\":\n",
    "            if group_sd is None or group_sd.size == 0:\n",
    "                # no update possible; keep previous beta (or init if first time)\n",
    "                beta = self._beta if self._beta_initialized else float(self.std_filter.beta_init)\n",
    "                beta = float(np.clip(beta, self.std_filter.beta_min, self.std_filter.beta_max))\n",
    "                return beta\n",
    "    \n",
    "            step_mean_sd = float(np.mean(group_sd))\n",
    "            if not self._beta_initialized:\n",
    "                self._beta = step_mean_sd\n",
    "                self._beta_initialized = True\n",
    "            else:\n",
    "                d = float(self.std_filter.ema_decay)\n",
    "                self._beta = d * self._beta + (1.0 - d) * step_mean_sd\n",
    "    \n",
    "            self._beta = float(np.clip(self._beta, self.std_filter.beta_min, self.std_filter.beta_max))\n",
    "            return float(self._beta)\n",
    "    \n",
    "        raise ValueError(f\"Unknown filtering_mode: {current_filtering_mode!r}\")\n",
    "    \n",
    "    def _std_filter_rewards(self, pr_np: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply RLPR std-based prompt filter.\n",
    "        - Computes group std over G generations\n",
    "        - Chooses beta (fixed or dynamic)\n",
    "        - Zeroes out groups with std < beta\n",
    "        \"\"\"\n",
    "        if not self.std_filter.enabled:\n",
    "            return pr_np\n",
    "    \n",
    "        G = int(self.std_filter.num_generations)\n",
    "        if G <= 1 or pr_np.shape[0] % G != 0:\n",
    "            return pr_np\n",
    "    \n",
    "        # 1) compute group std (beta=0 just to get sd; keep-all)\n",
    "        _, group_sd = _apply_rlpr_std_prompt_filter(\n",
    "                pr_np,\n",
    "                num_generations=G,\n",
    "                beta=0.0,\n",
    "                beta_min=self.std_filter.beta_min,\n",
    "                beta_max=self.std_filter.beta_max,\n",
    "                eps=self.std_filter.eps,\n",
    "        )\n",
    "    \n",
    "        # 2) choose beta (fixed or EMA-updated)\n",
    "        beta = self._get_current_beta(group_sd)\n",
    "    \n",
    "        # 3) apply filter with chosen beta\n",
    "        pr_np_filtered, _ = _apply_rlpr_std_prompt_filter(\n",
    "                pr_np,\n",
    "                num_generations=G,\n",
    "                beta=beta,\n",
    "                beta_min=self.std_filter.beta_min,\n",
    "                beta_max=self.std_filter.beta_max,\n",
    "                eps=self.std_filter.eps,\n",
    "        )\n",
    "        return pr_np_filtered        \n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        *,\n",
    "        prompts: List[str],\n",
    "        completions: List[str],\n",
    "        answer: Sequence[str],\n",
    "        **kwargs: Any,\n",
    "    ) -> List[float]:\n",
    "        answer_list = [str(a) for a in list(answer)]\n",
    "        if len(prompts) != len(completions) or len(prompts) != len(answer_list):\n",
    "            raise ValueError(\n",
    "                f\"Length mismatch: {len(prompts)=}, {len(completions)=}, {len(answer_list)=}\"\n",
    "            )\n",
    "\n",
    "        prompt_tok_base: List[np.ndarray] = []\n",
    "        prompt_tok_full: List[np.ndarray] = []\n",
    "        y_tok_list: List[List[int]] = []\n",
    "\n",
    "        for p, c, y in zip(prompts, completions, answer_list):\n",
    "            # prompt only\n",
    "            p_ids = _encode(self.tokenizer, p)\n",
    "            prompt_tok_base.append(_left_pad_to(np.asarray(p_ids, np.int32), self.max_prompt_length, self.pad_id))\n",
    "\n",
    "            # prompt + extracted reasoning (conditioning format for reward computation)\n",
    "            z = _extract_reasoning(c)\n",
    "            if z:\n",
    "                z_text = f\"<reasoning>{z}</reasoning>\"\n",
    "                full_text = p + \"\\n\" + z_text\n",
    "            else:\n",
    "                full_text = p\n",
    "            full_ids = _encode(self.tokenizer, full_text)\n",
    "            prompt_tok_full.append(_left_pad_to(np.asarray(full_ids, np.int32), self.max_prompt_length, self.pad_id))\n",
    "\n",
    "            # y* tokens (optionally wrap)\n",
    "            y_text = f\"<answer>{y}</answer>\" if self.wrap_y_with_answer_tags else y\n",
    "            y_ids = _encode(self.tokenizer, y_text)\n",
    "            if self.add_eos_to_y:\n",
    "                y_ids = y_ids + [self.eos_id]\n",
    "            y_tok_list.append(y_ids)\n",
    "\n",
    "        prompt_tok_base_j = jnp.asarray(np.stack(prompt_tok_base, axis=0), dtype=jnp.int32)\n",
    "        prompt_tok_full_j = jnp.asarray(np.stack(prompt_tok_full, axis=0), dtype=jnp.int32)\n",
    "\n",
    "        y_tok_np, y_mask_np = _right_pad_stack(y_tok_list, self.pad_id)\n",
    "        y_tok_j = jnp.asarray(y_tok_np, dtype=jnp.int32)\n",
    "        y_mask_j = jnp.asarray(y_mask_np, dtype=jnp.int32)\n",
    "\n",
    "       \n",
    "        actor_logps_full = self._actor_per_token_logps(prompt_tok_full_j, y_tok_j, y_mask_j)\n",
    "        r_full = _mean_prob_from_logps(actor_logps_full, y_mask_j)\n",
    "\n",
    "        if self.use_debias:\n",
    "            actor_logps_base = self._actor_per_token_logps(prompt_tok_base_j, y_tok_j, y_mask_j)\n",
    "            r_base = _mean_prob_from_logps(actor_logps_base, y_mask_j)  # [B]\n",
    "            pr_raw = r_full - r_base\n",
    "        else:\n",
    "            pr_raw = r_full\n",
    "\n",
    "        pr_clipped = jnp.clip(pr_raw, self.clip_min, self.clip_max)\n",
    "        pr_np = np.asarray(jax.device_get(pr_clipped), dtype=np.float32)\n",
    "        pr_np = self._std_filter_rewards(pr_np)\n",
    "\n",
    "        return pr_np.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RewardFn = Callable[..., List[float]]\n",
    "\n",
    "\n",
    "def _as_list_domain(x, n: int, default: str = \"\") -> list[str]:\n",
    "    if x is None:\n",
    "        return [default] * n\n",
    "\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = x.reshape(-1).tolist()\n",
    "\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        if len(x) != n:\n",
    "            raise ValueError(f\"domain length mismatch: {len(x)} vs batch {n}\")\n",
    "        return [str(v) for v in x]\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"domain must be a list/array of length {n} (got scalar {type(x).__name__})\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _slice(v: Any, idxs: list[int], B: int) -> Any:\n",
    "    \"\"\"If a kwarg is a length-B vector, slice it using idxs before forwarding\"\"\"\n",
    "    if v is None or isinstance(v, str):\n",
    "        return v\n",
    "    if isinstance(v, np.ndarray):\n",
    "        flat = v.reshape(-1)\n",
    "        return flat[idxs] if flat.shape[0] == B else v\n",
    "    if isinstance(v, (list, tuple)):\n",
    "        return [v[i] for i in idxs] if len(v) == B else v\n",
    "    return v\n",
    "\n",
    "\n",
    "def _infer_format_pass_value(rule_based_fn: Any, fallback: float = 3.0) -> float:\n",
    "    w = getattr(rule_based_fn, \"weights\", None)\n",
    "    if w is not None and hasattr(w, \"format_exact\"):\n",
    "        try:\n",
    "            return float(getattr(w, \"format_exact\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return float(fallback)\n",
    "\n",
    "\n",
    "class HybridRewardFn:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        rule_based_fn: Optional[RewardFn],\n",
    "        pr_fn: RewardFn,\n",
    "        w_rule: float = 1.0,\n",
    "        w_prob: float = 1.0,\n",
    "        rule_mode: str = \"format_exact+answer\",\n",
    "        name: str = \"hybrid_reward\",\n",
    "        apply_format_to_prob: bool = True,\n",
    "        w_format_prob: float = 1.0,\n",
    "        gate_prob_on_format: bool = False,\n",
    "        format_pass_value: Optional[float] = None,\n",
    "    ):\n",
    "        if pr_fn is None:\n",
    "            raise ValueError(\"pr_fn must be provided.\")\n",
    "        self.rule_based_fn = rule_based_fn\n",
    "        self.pr_fn = pr_fn\n",
    "        self.w_rule = float(w_rule)\n",
    "        self.w_prob = float(w_prob)\n",
    "        self.rule_mode = str(rule_mode)\n",
    "        self.__name__ = name\n",
    "\n",
    "        self.apply_format_to_prob = bool(apply_format_to_prob)\n",
    "        self.w_format_prob = float(w_format_prob)\n",
    "        self.gate_prob_on_format = bool(gate_prob_on_format)\n",
    "\n",
    "        if format_pass_value is not None:\n",
    "            self.format_pass_value = float(format_pass_value)\n",
    "        else:\n",
    "            self.format_pass_value = (\n",
    "                _infer_format_pass_value(rule_based_fn, fallback=3.0)\n",
    "                if rule_based_fn is not None\n",
    "                else 3.0\n",
    "            )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        *,\n",
    "        prompts: List[str],\n",
    "        completions: List[str],\n",
    "        answer: Sequence[str],\n",
    "        reward_model_type: Any = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[float]:\n",
    "\n",
    "        B = len(prompts)\n",
    "        if len(completions) != B or len(answer) != B:\n",
    "            raise ValueError(f\"Length mismatch: {len(prompts)=}, {len(completions)=}, {len(answer)=}\")\n",
    "\n",
    "        domains = _as_list_domain(kwargs.get(\"domain\", None), B, default=\"\")\n",
    "        domains = [d.strip().lower() for d in domains]\n",
    "\n",
    "        rule_idxs = [i for i, d in enumerate(domains) if d == \"math\"]\n",
    "        prob_idxs = [i for i in range(B) if i not in set(rule_idxs)]\n",
    "\n",
    "        out = np.zeros((B,), dtype=np.float32)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1) Rule group reward (math)\n",
    "        # -----------------------------\n",
    "        if rule_idxs:\n",
    "            if self.rule_based_fn is None:\n",
    "                raise ValueError(\"domain includes 'math' but rule_based_fn is None.\")\n",
    "\n",
    "            sub_kwargs = {k: _slice(v, rule_idxs, B) for k, v in kwargs.items()}\n",
    "            rule_r = self.rule_based_fn(\n",
    "                prompts=[prompts[i] for i in rule_idxs],\n",
    "                completions=[completions[i] for i in rule_idxs],\n",
    "                answer=[answer[i] for i in rule_idxs],\n",
    "                rule_mode=\"format_exact+answer\",\n",
    "                **sub_kwargs,\n",
    "            )\n",
    "            if len(rule_r) != len(rule_idxs):\n",
    "                raise RuntimeError(f\"rule_based_fn returned {len(rule_r)} != {len(rule_idxs)}\")\n",
    "\n",
    "            rule_np = np.asarray([float(x) for x in rule_r], dtype=np.float32)\n",
    "            for j, i in enumerate(rule_idxs):\n",
    "                out[i] = self.w_rule * float(rule_np[j])\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2) Prob(PR) group reward (non-math)\n",
    "        #    + optionally add format_exact\n",
    "        # -----------------------------\n",
    "        if prob_idxs:\n",
    "            if self.apply_format_to_prob and self.rule_based_fn is None:\n",
    "                raise ValueError(\"apply_format_to_prob=True but rule_based_fn is None.\")\n",
    "\n",
    "            sub_kwargs = {k: _slice(v, prob_idxs, B) for k, v in kwargs.items()}\n",
    "            \n",
    "            fmt_np = None\n",
    "            if self.apply_format_to_prob:\n",
    "                fmt_r = self.rule_based_fn(\n",
    "                    prompts=[prompts[i] for i in prob_idxs],\n",
    "                    completions=[completions[i] for i in prob_idxs],\n",
    "                    answer=[answer[i] for i in prob_idxs],  # format_exact ignores answer, but we pass it to keep batch lengths consistent\n",
    "                    rule_mode=\"format_exact\",\n",
    "                    **sub_kwargs,\n",
    "                )\n",
    "                if len(fmt_r) != len(prob_idxs):\n",
    "                    raise RuntimeError(f\"format_exact returned {len(fmt_r)} != {len(prob_idxs)}\")\n",
    "                fmt_np = np.asarray([float(x) for x in fmt_r], dtype=np.float32)\n",
    "\n",
    "            # Gating mode: if strict format fails, skip PR and apply only the format reward\n",
    "            if self.gate_prob_on_format and fmt_np is not None:\n",
    "                pass_val = self.format_pass_value\n",
    "\n",
    "                keep_local = [j for j in range(len(prob_idxs)) if float(fmt_np[j]) == pass_val]\n",
    "                drop_local = [j for j in range(len(prob_idxs)) if float(fmt_np[j]) != pass_val]\n",
    "\n",
    "                for j in drop_local:\n",
    "                    i = prob_idxs[j]\n",
    "                    out[i] = self.w_format_prob * float(fmt_np[j])\n",
    "                if keep_local:\n",
    "                    keep_idxs = [prob_idxs[j] for j in keep_local]\n",
    "                    keep_kwargs = {k: _slice(v, keep_idxs, B) for k, v in kwargs.items()}\n",
    "\n",
    "                    pr_r = self.pr_fn(\n",
    "                        prompts=[prompts[i] for i in keep_idxs],\n",
    "                        completions=[completions[i] for i in keep_idxs],\n",
    "                        answer=[answer[i] for i in keep_idxs],\n",
    "                        **keep_kwargs,\n",
    "                    )\n",
    "                    if len(pr_r) != len(keep_idxs):\n",
    "                        raise RuntimeError(f\"pr_fn returned {len(pr_r)} != {len(keep_idxs)}\")\n",
    "\n",
    "                    pr_np = np.asarray([float(x) for x in pr_r], dtype=np.float32)\n",
    "                    \n",
    "                    for t, j_local in enumerate(keep_local):\n",
    "                        i = prob_idxs[j_local]\n",
    "                        fmt_bonus = self.w_format_prob * float(fmt_np[j_local])\n",
    "                        out[i] = self.w_prob * float(pr_np[t]) + fmt_bonus\n",
    "\n",
    "                return out.tolist()\n",
    "            # Additive mode: compute PR for all samples and add the format bonus    \n",
    "            pr_r = self.pr_fn(\n",
    "                prompts=[prompts[i] for i in prob_idxs],\n",
    "                completions=[completions[i] for i in prob_idxs],\n",
    "                answer=[answer[i] for i in prob_idxs],\n",
    "                **sub_kwargs,\n",
    "            )\n",
    "            if len(pr_r) != len(prob_idxs):\n",
    "                raise RuntimeError(f\"pr_fn returned {len(pr_r)} != {len(prob_idxs)}\")\n",
    "\n",
    "            pr_np = np.asarray([float(x) for x in pr_r], dtype=np.float32)\n",
    "\n",
    "\n",
    "            for j, i in enumerate(prob_idxs):\n",
    "                fmt_bonus = self.w_format_prob * float(fmt_np[j]) if fmt_np is not None else 0.0\n",
    "                out[i] = self.w_prob * float(pr_np[j]) + fmt_bonus\n",
    "\n",
    "        return out.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Load Gemma2-2B-it Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_hbm_usage():\n",
    "  \"\"\"Displays memory usage per device.\"\"\"\n",
    "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "\n",
    "  for d in jax.local_devices():\n",
    "    stats = d.memory_stats()\n",
    "    used = stats[\"bytes_in_use\"]\n",
    "    limit = stats[\"bytes_limit\"]\n",
    "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = params_lib.load_and_format_params(\n",
    "      os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\")\n",
    "  )\n",
    "gemma = model_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer.save(os.path.join(SFT_INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "checkpointer.wait_until_finished()\n",
    "# Delete the intermediate model to save memory.\n",
    "del params\n",
    "del gemma\n",
    "del state\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_gemma_base_model(ckpt_path):\n",
    "  mesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[1]))\n",
    "  model_config = model_lib.ModelConfig.gemma2_2b()\n",
    "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "      lambda: model_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "  )\n",
    "  abs_state = nnx.state(abs_gemma)\n",
    "  abs_state = jax.tree.map(\n",
    "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "      abs_state,\n",
    "      nnx.get_named_sharding(abs_state, mesh),\n",
    "  )\n",
    "  checkpointer = ocp.StandardCheckpointer()\n",
    "  restored_params = checkpointer.restore(os.path.join(ckpt_path, \"state\"), target=abs_state)\n",
    "\n",
    "  graph_def, _ = nnx.split(abs_gemma)\n",
    "  gemma = nnx.merge(graph_def, restored_params)\n",
    "  return gemma, mesh, model_config\n",
    "\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "  lora_provider = qwix.LoraProvider(\n",
    "      module_path=(\n",
    "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "          \".*attn_vec_einsum\"\n",
    "      ),\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "  )\n",
    "\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, **model_input\n",
    "  )\n",
    "\n",
    "  with mesh:\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Base model\n",
    "base_model, mesh, model_config = get_gemma_base_model(SFT_INTERMEDIATE_CKPT_DIR)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_policy = get_lora_model(base_model, mesh=mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- HBM Usage After Model Load ---\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "os.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ SFT Stage (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sft_checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=SFT_SAVE_INTERVAL_STEPS, max_to_keep=SFT_MAX_TO_KEEP\n",
    ")\n",
    "\n",
    "sft_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "        log_dir=SFT_LOG_DIR, flush_every_n_steps=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sft_optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=SFT_LEARNING_RATE,\n",
    "        warmup_steps=SFT_WARMUP_STEPS,\n",
    "        decay_steps=max(SFT_MAX_STEPS, 1),\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    weight_decay=SFT_WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "if SFT_MAX_GRAD_NORM is not None:\n",
    "  sft_optimizer = optax.chain(\n",
    "      optax.clip_by_global_norm(max_norm=SFT_MAX_GRAD_NORM),\n",
    "      sft_optimizer,\n",
    "  )\n",
    "\n",
    " \n",
    "sft_training_config = peft_trainer.TrainingConfig(\n",
    "    eval_every_n_steps=SFT_EVAL_EVERY_N_STEPS,\n",
    "    max_steps=SFT_MAX_STEPS,\n",
    "    gradient_accumulation_steps=SFT_GRAD_ACCUM_STEPS,\n",
    "    checkpoint_root_directory=SFT_CKPT_DIR,\n",
    "    checkpointing_options=sft_checkpointing_options,\n",
    "    metrics_logging_options=sft_logging_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sft_trainer = peft_trainer.PeftTrainer(\n",
    "        model=lora_policy,\n",
    "        optimizer=sft_optimizer,\n",
    "        training_config=sft_training_config,\n",
    "    ).with_gen_model_input_fn(gen_model_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with mesh:\n",
    "    sft_trainer.train(sft_train_dataset, sft_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ SFT Stage (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper function to load ckpt\n",
    "def get_latest_step(ckpt_root: str) -> int:\n",
    "    steps = []\n",
    "    for name in os.listdir(ckpt_root):\n",
    "        path = os.path.join(ckpt_root, name)\n",
    "        if name.isdigit() and os.path.isdir(path):\n",
    "            steps.append(int(name))\n",
    "    if not steps:\n",
    "        raise ValueError(\"No checkpoint steps found\")\n",
    "    return max(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"tunix-SFT-2\")\n",
    "\n",
    "latest_step = get_latest_step(SFT_CKPT_DIR)\n",
    "print(f\"Load {latest_step} step Ckpt\")\n",
    "sft_stage1_ckpt_dir =  os.path.join(\n",
    "    SFT_CKPT_DIR, str(latest_step), \"model_params\"\n",
    ")\n",
    "\n",
    "abs_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(sft_stage1_ckpt_dir, target=abs_params)\n",
    "\n",
    "with mesh:\n",
    "    nnx.update(\n",
    "        lora_policy,\n",
    "        jax.tree.map(\n",
    "            lambda a, b: b,\n",
    "            nnx.state(lora_policy, nnx.LoRAParam),\n",
    "            trained_lora_params,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sft_stage2_optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=SFT_STAGE2_LEARNING_RATE,\n",
    "        warmup_steps=int(0.1 * SFT_STAGE2_MAX_STEPS),\n",
    "        decay_steps=max(SFT_STAGE2_MAX_STEPS, 1),\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    weight_decay=SFT_WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "if SFT_MAX_GRAD_NORM is not None:\n",
    "  sft_stage2_optimizer = optax.chain(\n",
    "      optax.clip_by_global_norm(max_norm=SFT_MAX_GRAD_NORM),\n",
    "      sft_stage2_optimizer,\n",
    "  )\n",
    "\n",
    " \n",
    "sft_stage2_training_config = peft_trainer.TrainingConfig(\n",
    "    eval_every_n_steps=SFT_EVAL_EVERY_N_STEPS,\n",
    "    max_steps=SFT_STAGE2_MAX_STEPS,\n",
    "    gradient_accumulation_steps=SFT_GRAD_ACCUM_STEPS,\n",
    "    checkpoint_root_directory=SFT_STAGE2_CKPT_DIR,\n",
    "    checkpointing_options=sft_checkpointing_options,\n",
    "    metrics_logging_options=sft_logging_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sft_stage2_trainer = peft_trainer.PeftTrainer(\n",
    "        model=lora_policy,\n",
    "        optimizer=sft_stage2_optimizer,\n",
    "        training_config=sft_stage2_training_config,\n",
    "    ).with_gen_model_input_fn(gen_model_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with mesh:\n",
    "    sft_stage2_trainer.train(sft_stage2_train_dataset, sft_stage2_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ RL Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"tunix-RL\")\n",
    "\n",
    "latest_step = get_latest_step(SFT_STAGE2_CKPT_DIR)\n",
    "\n",
    "sft_two_trained_ckpt_dir = f\"{SFT_STAGE2_CKPT_DIR}/{latest_step}/model_params\"\n",
    "\n",
    "abs_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(sft_two_trained_ckpt_dir, target=abs_params)\n",
    "with mesh:\n",
    "    nnx.update(\n",
    "        lora_policy,\n",
    "        jax.tree.map(\n",
    "            lambda a, b: b,\n",
    "            nnx.state(lora_policy, nnx.LoRAParam),\n",
    "            trained_lora_params,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=RL_SAVE_INTERVAL_STEPS, max_to_keep=RL_MAX_TO_KEEP\n",
    ")\n",
    "\n",
    "# Metrics logger\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=RL_LEARNING_RATE,\n",
    "        warmup_steps=RL_WARMUP_STEPS,\n",
    "        decay_steps=RL_MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=RL_B1,\n",
    "    b2=RL_B2,\n",
    "    weight_decay=RL_WEIGHT_DECAY,\n",
    ")\n",
    "if RL_MAX_GRAD_NORM is not None:\n",
    "  optimizer = optax.chain(\n",
    "      optax.clip_by_global_norm(max_norm=RL_MAX_GRAD_NORM),\n",
    "      optimizer,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine='vanilla',\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=RL_EVAL_EVERY_N_STEPS,\n",
    "        max_steps=RL_MAX_STEPS,\n",
    "        mini_batch_size=RL_TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=RL_TRAIN_MICRO_BATCH_SIZE,\n",
    "        # metrics logging\n",
    "        metrics_logging_options=metrics_logging_options,\n",
    "        # checkpoint saving\n",
    "        checkpoint_root_directory=FINAL_CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=RL_TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=RL_MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=RL_MAX_PROMPT_LENGTH + RL_TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=RL_TEMPERATURE,\n",
    "        top_p=RL_TOP_P,\n",
    "        top_k=RL_TOP_K,\n",
    "        eos_tokens=[1,106],\n",
    "    ),\n",
    ")\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=RL_NUM_GENERATIONS,\n",
    "    num_iterations=RL_NUM_ITERATIONS,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=lora_policy,\n",
    "    reference=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pr_fn = ProbabilityBasedRewardFn(\n",
    "    rl_cluster=rl_cluster,\n",
    "    tokenizer=tokenizer,\n",
    "    use_debias=True,\n",
    "    std_filter=StdFilterConfig(\n",
    "    enabled=True,\n",
    "    num_generations=4,\n",
    "    filtering_mode=\"dynamic\",\n",
    "    beta_init=0.0,\n",
    "),)\n",
    "\n",
    "rule_fn = RuleBasedRewardFn()\n",
    "\n",
    "\n",
    "hybrid = HybridRewardFn(\n",
    "    rule_based_fn=rule_fn,\n",
    "    pr_fn=pr_fn,\n",
    "    w_rule=1.0,\n",
    "    w_prob=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[\n",
    "hybrid\n",
    "    ],\n",
    "    grpo_config=grpo_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with mesh:\n",
    "  grpo_trainer.train(rl_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Train from CKPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "USE_CKPT = True\n",
    "ckpt_from = \"username/modelname\"\n",
    "\n",
    "if USE_CKPT:\n",
    "    !kaggle kernels output {ckpt_from} -p /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_gemma_base_model_from_ckpt(ckpt_path):\n",
    "    mesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[1]))\n",
    "    model_config = model_lib.ModelConfig.gemma2_2b()\n",
    "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "      lambda: model_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "      abs_state,\n",
    "      nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(os.path.join(ckpt_path, \"intermediate_ckpt\",\"state\"), target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    \n",
    "    return gemma, mesh, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# del base_model\n",
    "# del lora_policy\n",
    "# gc.collect()\n",
    "\n",
    "print(\"\\n--- HBM Usage BEFORE Model Load ---\")\n",
    "show_hbm_usage()\n",
    "\n",
    "local_ckpt_dir = \"/kaggle/working/ckpts\"\n",
    "\n",
    "base_model, mesh, model_config = get_gemma_base_model_from_ckpt(local_ckpt_dir)\n",
    "lora_policy = get_lora_model(base_model, mesh=mesh)\n",
    "\n",
    "def get_latest_step(ckpt_root: str) -> int:\n",
    "    steps = []\n",
    "    for name in os.listdir(ckpt_root):\n",
    "        path = os.path.join(ckpt_root, name)\n",
    "        if name.isdigit() and os.path.isdir(path):\n",
    "            steps.append(int(name))\n",
    "    if not steps:\n",
    "        raise ValueError(\"No checkpoint steps found\")\n",
    "    return max(steps)\n",
    "\n",
    "actor_dir = os.path.join(local_ckpt_dir, \"actor\")\n",
    "latest_step = get_latest_step(actor_dir) \n",
    "local_ckpt_dir = os.path.join(actor_dir, str(latest_step),\"model_params\") #FINAL_CKPT_DIR, \"actor\",str(100), \"model_params\"\n",
    "\n",
    "\n",
    "abs_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(local_ckpt_dir, target=abs_params)\n",
    "with mesh:\n",
    "    nnx.update(\n",
    "        lora_policy,\n",
    "        jax.tree.map(\n",
    "            lambda a, b: b,\n",
    "            nnx.state(lora_policy, nnx.LoRAParam),\n",
    "            trained_lora_params,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(\"\\n--- HBM Usage AFTER Model Load ---\")\n",
    "\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with mesh:\n",
    "  grpo_trainer.train(rl_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrestricted mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unrestricted_kaggle_model = \"sangminlee09/dod-gemma2-2b-it-reasoning-800\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Other things we want the judges to know\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Lesson Learned\n",
    "**CoT Length**\n",
    "\n",
    "Long chain-of-thought (CoT) data naturally encode advanced reasoning behaviors‚Äîself-reflection, verification, and dynamic strategy adaptation‚Äîthat are typically absent from short CoT.\n",
    "\n",
    "However, for small models (at least in the LoRA-based SFT setting with a ~20k dataset), long CoTs are difficult to learn effectively. We observe that the model tends to overthink even on easy problems (e.g., GSM8K), producing unnecessarily long reasoning traces and still arriving at incorrect answers. On such easy tasks, short CoT training often performs better, yielding more stable and accurate solutions. At the same time, training only on short CoT limits the model‚Äôs reasoning capacity: it fails to acquire the higher-order abilities that long CoT enables, such as extended planning and iterative verification.\n",
    "\n",
    "Therefore, mixing long and short CoT in an appropriate ratio appears crucial during SFT as a pre-training stage for subsequent RL. Short CoT helps anchor correctness and efficiency on simple tasks, while long CoT exposes the model to richer reasoning patterns that can be further amplified and refined during RL.\n",
    "\n",
    "**Data Domain Ratio**\n",
    "\n",
    "Verifiable data provides stable anchors for correctness, while non-verifiable data enables broad general reasoning. We aim to maximize the proportion of general-domain data while maintaining (or at least not degrading) basic mathematical ability, even if improvements are only marginal. In this stage, enforcing a stable reasoning format is also critical. We therefore tune the dataset mixture by monitoring GSM8K accuracy as a proxy for core math competence and format accuracy to ensure consistent adherence to the required output structure in the SFT stage.\n",
    "\n",
    "**Make Reasoning Consice and Compact**\n",
    "\n",
    "Concise and compact reasoning does not emerge from aggressive truncation alone.\n",
    "Instead, it arises from a staged process in which the model is first exposed to rich reasoning behaviors, then further learn more at learnable reasoning lengths, and finally optimized under strict generation constraints, together with verified rewards, implicitly favor efficient reasoning by encouraging correctness within limited generation budgets.\n",
    "\n",
    "\n",
    "üí¨ **Comment**\n",
    "- The integration with Weights & Biases (W&B) made experiment tracking and iteration straightforward and convenient.\n",
    "- Tighter integration with standard LLM benchmarks would be highly beneficial, making the framework a much stronger post-training solution.\n",
    "- Accessing TPUs required long wait times, which significantly slowed down experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the final checkpoint for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CKPT_DIR = FINAL_CKPT_DIR\n",
    "\n",
    "import re\n",
    "\n",
    "# Find the latest checkpoint by listing directories in CKPT_DIR/actor\n",
    "actor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n",
    "\n",
    "latest_step = -1\n",
    "if os.path.exists(actor_ckpt_dir):\n",
    "  for item in os.listdir(actor_ckpt_dir):\n",
    "    if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n",
    "      step = int(item)\n",
    "      if step > latest_step:\n",
    "        latest_step = step\n",
    "\n",
    "if latest_step == -1:\n",
    "  raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n",
    "\n",
    "print(f\"Latest checkpoint step: {latest_step}\")\n",
    "\n",
    "wandb.init(project='tunix-eval')  # logging bug workaround\n",
    "\n",
    "loaded_ckpt_path = os.path.join(\n",
    "    CKPT_DIR, \"actor\", str(latest_step), \"model_params\"\n",
    ")\n",
    "\n",
    "abs_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(loaded_ckpt_path, target=abs_params)\n",
    "\n",
    "nnx.update(\n",
    "    lora_policy,\n",
    "    jax.tree.map(\n",
    "        lambda a, b: b,\n",
    "        nnx.state(lora_policy, nnx.LoRAParam),\n",
    "        trained_lora_params,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the sampler for finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=MAX_PROMPT_LENGTH + MAX_GENERATION_STEPS + 256,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TunixHackathonJudge:\n",
    "    questions = ['question1', 'question2', ...]\n",
    "    judge = \"ai\"\n",
    "    \n",
    "    def __init__(self, temperature, top_k, top_p, max_generation_steps, seed):\n",
    "        ...\n",
    "\n",
    "    def evaluate(self, sampler, prompt):\n",
    "        ...\n",
    "\n",
    "Result = TunixHackathonJudge(INF_TEMPERATURE, INF_TOP_K, INF_TOP_P, MAX_GENERATION_STEPS, SEED).evaluate(sampler, PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unrestricted mode (multi-session mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unrestricted_kaggle_model = \"sangminlee09/dod-gemma2-2b-it-reasoning-800\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9225227,
     "sourceId": 14442512,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9228098,
     "sourceId": 14446760,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9233248,
     "sourceId": 14455808,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72251,
     "sourceId": 85992,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
